{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6735188,"sourceType":"datasetVersion","datasetId":3878683}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Required libraries\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Subset\n\n# Set device (GPU/CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load dataset\ndata_dir = '/kaggle/input/defungi'  # Change this to your dataset path\n\n# Define basic transformations\ntransform = transforms.Compose([\n    transforms.Resize((64,64)),\n    transforms.ToTensor(),\n])\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n\n# Split data into labeled (small fraction) and unlabeled\ntrain_indices, test_indices = train_test_split(list(range(len(full_dataset))), test_size=0.2, stratify=full_dataset.targets)\nlabeled_indices, unlabeled_indices = train_test_split(train_indices, test_size=0.9, stratify=np.array(full_dataset.targets)[train_indices])\n\nlabeled_dataset = Subset(full_dataset, labeled_indices)\nunlabeled_dataset = Subset(full_dataset, unlabeled_indices)\ntest_dataset = Subset(full_dataset, test_indices)\n\n# DataLoaders\nbatch_size = 32\nlabeled_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\nunlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define CNN model\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n        self.fc2 = nn.Linear(128, 6)  # 6 fungi classes\n\n    def forward(self, x):\n        x = self.pool(nn.ReLU()(self.conv1(x)))\n        x = self.pool(nn.ReLU()(self.conv2(x)))\n        x = x.view(-1, 64 * 16 * 16)\n        x = nn.ReLU()(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Define training and evaluation function\ndef train_model(model, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for (labeled_images, labeled_targets), (unlabeled_images, _) in zip(labeled_loader, unlabeled_loader):\n            labeled_images, labeled_targets = labeled_images.to(device), labeled_targets.to(device)\n            unlabeled_images = unlabeled_images.to(device)\n\n            optimizer.zero_grad()\n\n            # Forward pass on labeled data\n            labeled_outputs = model(labeled_images)\n            labeled_loss = criterion(labeled_outputs, labeled_targets)\n\n            # Forward pass on unlabeled data\n            pseudo_labels = model(unlabeled_images).detach().argmax(dim=1)\n            unlabeled_outputs = model(unlabeled_images)\n            unlabeled_loss = criterion(unlabeled_outputs, pseudo_labels)\n\n            # Combined loss\n            loss = labeled_loss + unlabeled_loss\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(labeled_loader):.4f}')\n\n# FixMatch model (consistency regularization)\nclass FixMatch(nn.Module):\n    def __init__(self, model, tau=0.95):\n        super(FixMatch, self).__init__()\n        self.model = model\n        self.tau = tau\n\n    def forward(self, x_weak, x_strong):\n        logits_weak = self.model(x_weak)\n        pseudo_labels = torch.softmax(logits_weak, dim=1).argmax(dim=1)\n        mask = torch.max(torch.softmax(logits_weak, dim=1), dim=1)[0] >= self.tau\n        logits_strong = self.model(x_strong)\n        return logits_strong, pseudo_labels, mask\n\n# Mean Teacher model\nclass MeanTeacher(nn.Module):\n    def __init__(self, student, teacher, alpha=0.99):\n        super(MeanTeacher, self).__init__()\n        self.student = student\n        self.teacher = teacher\n        self.alpha = alpha\n\n    def update_teacher(self):\n        for teacher_param, student_param in zip(self.teacher.parameters(), self.student.parameters()):\n            teacher_param.data = self.alpha * teacher_param.data + (1 - self.alpha) * student_param.data\n\n    def forward(self, x):\n        return self.student(x), self.teacher(x)\n\n# MixMatch augmentation\ndef mixmatch(x_labeled, y_labeled, x_unlabeled, model, T=0.5, K=2):\n    with torch.no_grad():\n        preds_unlabeled = [model(x_unlabeled) for _ in range(K)]\n        preds_unlabeled = torch.mean(torch.stack(preds_unlabeled), dim=0)\n        pseudo_labels = torch.softmax(preds_unlabeled / T, dim=1)\n    all_inputs = torch.cat([x_labeled, x_unlabeled], dim=0)\n    all_labels = torch.cat([y_labeled, pseudo_labels.argmax(dim=1)], dim=0)\n    lam = np.random.beta(0.75, 0.75)\n    index = torch.randperm(all_inputs.size(0))\n    mixed_inputs = lam * all_inputs + (1 - lam) * all_inputs[index]\n    mixed_labels = lam * all_labels + (1 - lam) * all_labels[index]\n    return mixed_inputs, mixed_labels\n\n# Model evaluation\ndef evaluate_model(model, test_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))\n\n# Train and evaluate FixMatch model\ncnn_model = CNN().to(device)\nfixmatch_model = FixMatch(cnn_model).to(device)\noptimizer = optim.Adam(fixmatch_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_model(fixmatch_model.model, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=10)\nprint(\"FixMatch Model Evaluation:\")\nevaluate_model(fixmatch_model.model, test_loader)\n\n# Train and evaluate Mean Teacher model\nstudent_model = CNN().to(device)\nteacher_model = CNN().to(device)\nmean_teacher_model = MeanTeacher(student_model, teacher_model).to(device)\n\ntrain_model(mean_teacher_model.student, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=10)\nprint(\"Mean Teacher Model Evaluation:\")\nevaluate_model(mean_teacher_model.student, test_loader)\n\n# Train and evaluate MixMatch model\ntrain_model(cnn_model, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=10)\nprint(\"MixMatch Model Evaluation:\")\nevaluate_model(cnn_model, test_loader)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-27T19:23:38.631946Z","iopub.execute_input":"2024-09-27T19:23:38.632311Z","iopub.status.idle":"2024-09-27T19:26:32.633366Z","shell.execute_reply.started":"2024-09-27T19:23:38.632276Z","shell.execute_reply":"2024-09-27T19:26:32.632405Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 2.7636\nEpoch [2/10], Loss: 2.1165\nEpoch [3/10], Loss: 1.6412\nEpoch [4/10], Loss: 1.5608\nEpoch [5/10], Loss: 1.5506\nEpoch [6/10], Loss: 1.5252\nEpoch [7/10], Loss: 1.4928\nEpoch [8/10], Loss: 1.4576\nEpoch [9/10], Loss: 1.4836\nEpoch [10/10], Loss: 1.4492\nFixMatch Model Evaluation:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          H1       0.63      0.97      0.76       881\n          H2       0.38      0.18      0.25       467\n          H3       0.00      0.00      0.00       164\n          H5       0.00      0.00      0.00       163\n          H6       0.48      0.78      0.59       148\n\n    accuracy                           0.58      1823\n   macro avg       0.30      0.39      0.32      1823\nweighted avg       0.44      0.58      0.48      1823\n\nEpoch [1/10], Loss: 3.4397\nEpoch [2/10], Loss: 3.4396\nEpoch [3/10], Loss: 3.4396\nEpoch [4/10], Loss: 3.4396\nEpoch [5/10], Loss: 3.4396\nEpoch [6/10], Loss: 3.4400\nEpoch [7/10], Loss: 3.4397\nEpoch [8/10], Loss: 3.4396\nEpoch [9/10], Loss: 3.4396\nEpoch [10/10], Loss: 3.4395\nMean Teacher Model Evaluation:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          H1       0.48      1.00      0.65       881\n          H2       0.00      0.00      0.00       467\n          H3       0.00      0.00      0.00       164\n          H5       0.00      0.00      0.00       163\n          H6       0.00      0.00      0.00       148\n\n    accuracy                           0.48      1823\n   macro avg       0.10      0.20      0.13      1823\nweighted avg       0.23      0.48      0.31      1823\n\nEpoch [1/10], Loss: 1.4153\nEpoch [2/10], Loss: 1.4562\nEpoch [3/10], Loss: 1.5305\nEpoch [4/10], Loss: 1.3876\nEpoch [5/10], Loss: 1.4492\nEpoch [6/10], Loss: 1.4108\nEpoch [7/10], Loss: 1.3506\nEpoch [8/10], Loss: 1.3144\nEpoch [9/10], Loss: 1.3087\nEpoch [10/10], Loss: 1.2907\nMixMatch Model Evaluation:\n              precision    recall  f1-score   support\n\n          H1       0.62      0.97      0.76       881\n          H2       0.47      0.13      0.21       467\n          H3       0.45      0.06      0.11       164\n          H5       0.60      0.48      0.53       163\n          H6       0.67      0.70      0.68       148\n\n    accuracy                           0.61      1823\n   macro avg       0.56      0.47      0.46      1823\nweighted avg       0.57      0.61      0.53      1823\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Required libraries\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Subset\n\n# Set device (GPU/CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load dataset\ndata_dir = '/kaggle/input/defungi'  # Change this to your dataset path\n\n# Define basic transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # ResNet input size is 224x224\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization for pre-trained models\n])\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n\n# Split data into labeled (small fraction) and unlabeled\ntrain_indices, test_indices = train_test_split(list(range(len(full_dataset))), test_size=0.2, stratify=full_dataset.targets)\nlabeled_indices, unlabeled_indices = train_test_split(train_indices, test_size=0.9, stratify=np.array(full_dataset.targets)[train_indices])\n\nlabeled_dataset = Subset(full_dataset, labeled_indices)\nunlabeled_dataset = Subset(full_dataset, unlabeled_indices)\ntest_dataset = Subset(full_dataset, test_indices)\n\n# DataLoaders\nbatch_size = 32\nlabeled_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\nunlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Load Pretrained Model (ResNet18)\ndef get_pretrained_model():\n    model = models.resnet18(pretrained=True)\n    # Modify the final layer to match the number of fungi classes (6 classes in your case)\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, 6)  # 6 fungi classes\n    return model\n\n# Define training and evaluation function\ndef train_model(model, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for (labeled_images, labeled_targets), (unlabeled_images, _) in zip(labeled_loader, unlabeled_loader):\n            labeled_images, labeled_targets = labeled_images.to(device), labeled_targets.to(device)\n            unlabeled_images = unlabeled_images.to(device)\n\n            optimizer.zero_grad()\n\n            # Forward pass on labeled data\n            labeled_outputs = model(labeled_images)\n            labeled_loss = criterion(labeled_outputs, labeled_targets)\n\n            # Forward pass on unlabeled data\n            pseudo_labels = model(unlabeled_images).detach().argmax(dim=1)\n            unlabeled_outputs = model(unlabeled_images)\n            unlabeled_loss = criterion(unlabeled_outputs, pseudo_labels)\n\n            # Combined loss\n            loss = labeled_loss + unlabeled_loss\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(labeled_loader):.4f}')\n\n# FixMatch model (consistency regularization)\nclass FixMatch(nn.Module):\n    def __init__(self, model, tau=0.95):\n        super(FixMatch, self).__init__()\n        self.model = model\n        self.tau = tau\n\n    def forward(self, x_weak, x_strong):\n        logits_weak = self.model(x_weak)\n        pseudo_labels = torch.softmax(logits_weak, dim=1).argmax(dim=1)\n        mask = torch.max(torch.softmax(logits_weak, dim=1), dim=1)[0] >= self.tau\n        logits_strong = self.model(x_strong)\n        return logits_strong, pseudo_labels, mask\n\n# Mean Teacher model\nclass MeanTeacher(nn.Module):\n    def __init__(self, student, teacher, alpha=0.99):\n        super(MeanTeacher, self).__init__()\n        self.student = student\n        self.teacher = teacher\n        self.alpha = alpha\n\n    def update_teacher(self):\n        for teacher_param, student_param in zip(self.teacher.parameters(), self.student.parameters()):\n            teacher_param.data = self.alpha * teacher_param.data + (1 - self.alpha) * student_param.data\n\n    def forward(self, x):\n        return self.student(x), self.teacher(x)\n\n# MixMatch augmentation\ndef mixmatch(x_labeled, y_labeled, x_unlabeled, model, T=0.5, K=2):\n    with torch.no_grad():\n        preds_unlabeled = [model(x_unlabeled) for _ in range(K)]\n        preds_unlabeled = torch.mean(torch.stack(preds_unlabeled), dim=0)\n        pseudo_labels = torch.softmax(preds_unlabeled / T, dim=1)\n    all_inputs = torch.cat([x_labeled, x_unlabeled], dim=0)\n    all_labels = torch.cat([y_labeled, pseudo_labels.argmax(dim=1)], dim=0)\n    lam = np.random.beta(0.75, 0.75)\n    index = torch.randperm(all_inputs.size(0))\n    mixed_inputs = lam * all_inputs + (1 - lam) * all_inputs[index]\n    mixed_labels = lam * all_labels + (1 - lam) * all_labels[index]\n    return mixed_inputs, mixed_labels\n\n# Model evaluation\ndef evaluate_model(model, test_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))\n\n# Instantiate the pre-trained model\npretrained_model = get_pretrained_model().to(device)\n\n# Train and evaluate FixMatch model\nfixmatch_model = FixMatch(pretrained_model).to(device)\noptimizer = optim.Adam(fixmatch_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_model(fixmatch_model.model, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=32)\nprint(\"FixMatch Model Evaluation:\")\nevaluate_model(fixmatch_model.model, test_loader)\n\n# # Train and evaluate Mean Teacher model\n# student_model = get_pretrained_model().to(device)\n# teacher_model = get_pretrained_model().to(device)\n# mean_teacher_model = MeanTeacher(student_model, teacher_model).to(device)\n\n# train_model(mean_teacher_model.student, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=5)\n# print(\"Mean Teacher Model Evaluation:\")\n# evaluate_model(mean_teacher_model.student, test_loader)\n\n# Train and evaluate MixMatch model\ntrain_model(pretrained_model, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=32)\nprint(\"MixMatch Model Evaluation:\")\nevaluate_model(pretrained_model, test_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:48:05.404468Z","iopub.execute_input":"2024-09-27T19:48:05.404995Z","iopub.status.idle":"2024-09-27T19:48:06.859410Z","shell.execute_reply.started":"2024-09-27T19:48:05.404949Z","shell.execute_reply":"2024-09-27T19:48:06.858174Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n100%|██████████| 35.2M/35.2M [00:00<00:00, 192MB/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(all_labels, all_preds, target_names\u001b[38;5;241m=\u001b[39mfull_dataset\u001b[38;5;241m.\u001b[39mclasses))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Instantiate the pre-trained model\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Train and evaluate FixMatch model\u001b[39;00m\n\u001b[1;32m    140\u001b[0m fixmatch_model \u001b[38;5;241m=\u001b[39m FixMatch(pretrained_model)\u001b[38;5;241m.\u001b[39mto(device)\n","Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mget_pretrained_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mefficientnet_b2(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Modify the final layer to match the number of fungi classes (6 classes in your case)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m num_ftrs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[38;5;241m.\u001b[39min_features\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, \u001b[38;5;241m6\u001b[39m)  \u001b[38;5;66;03m# 6 fungi classes\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'EfficientNet' object has no attribute 'fc'"],"ename":"AttributeError","evalue":"'EfficientNet' object has no attribute 'fc'","output_type":"error"}]},{"cell_type":"code","source":"# Full Code with EfficientNet\n# Required libraries\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Subset\n\n# Set device (GPU/CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load dataset\ndata_dir = '/kaggle/input/defungi'  # Change this to your dataset path\n\n# Define basic transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # EfficientNet input size is 224x224\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization for pre-trained models\n])\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n\n# Split data into labeled (small fraction) and unlabeled\ntrain_indices, test_indices = train_test_split(list(range(len(full_dataset))), test_size=0.2, stratify=full_dataset.targets)\nlabeled_indices, unlabeled_indices = train_test_split(train_indices, test_size=0.9, stratify=np.array(full_dataset.targets)[train_indices])\n\nlabeled_dataset = Subset(full_dataset, labeled_indices)\nunlabeled_dataset = Subset(full_dataset, unlabeled_indices)\ntest_dataset = Subset(full_dataset, test_indices)\n\n# DataLoaders\nbatch_size = 32\nlabeled_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\nunlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Load Pretrained Model (EfficientNet)\ndef get_pretrained_model():\n    model = models.efficientnet_b2(pretrained=True)  # Use EfficientNet B0\n    num_ftrs = model.classifier[1].in_features  # Access the first layer of the classifier\n    model.classifier[1] = nn.Linear(num_ftrs, 6)  # 6 fungi classes\n    return model\n\n# Define training and evaluation function\ndef train_model(model, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for (labeled_images, labeled_targets), (unlabeled_images, _) in zip(labeled_loader, unlabeled_loader):\n            labeled_images, labeled_targets = labeled_images.to(device), labeled_targets.to(device)\n            unlabeled_images = unlabeled_images.to(device)\n\n            optimizer.zero_grad()\n\n            # Forward pass on labeled data\n            labeled_outputs = model(labeled_images)\n            labeled_loss = criterion(labeled_outputs, labeled_targets)\n\n            # Forward pass on unlabeled data\n            pseudo_labels = model(unlabeled_images).detach().argmax(dim=1)\n            unlabeled_outputs = model(unlabeled_images)\n            unlabeled_loss = criterion(unlabeled_outputs, pseudo_labels)\n\n            # Combined loss\n            loss = labeled_loss + unlabeled_loss\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(labeled_loader):.4f}')\n\n# FixMatch model (consistency regularization)\nclass FixMatch(nn.Module):\n    def __init__(self, model, tau=0.95):\n        super(FixMatch, self).__init__()\n        self.model = model\n        self.tau = tau\n\n    def forward(self, x_weak, x_strong):\n        logits_weak = self.model(x_weak)\n        pseudo_labels = torch.softmax(logits_weak, dim=1).argmax(dim=1)\n        mask = torch.max(torch.softmax(logits_weak, dim=1), dim=1)[0] >= self.tau\n        logits_strong = self.model(x_strong)\n        return logits_strong, pseudo_labels, mask\n\n# Mean Teacher model\nclass MeanTeacher(nn.Module):\n    def __init__(self, student, teacher, alpha=0.99):\n        super(MeanTeacher, self).__init__()\n        self.student = student\n        self.teacher = teacher\n        self.alpha = alpha\n\n    def update_teacher(self):\n        for teacher_param, student_param in zip(self.teacher.parameters(), self.student.parameters()):\n            teacher_param.data = self.alpha * teacher_param.data + (1 - self.alpha) * student_param.data\n\n    def forward(self, x):\n        return self.student(x), self.teacher(x)\n\n# MixMatch augmentation\ndef mixmatch(x_labeled, y_labeled, x_unlabeled, model, T=0.5, K=2):\n    with torch.no_grad():\n        preds_unlabeled = [model(x_unlabeled) for _ in range(K)]\n        preds_unlabeled = torch.mean(torch.stack(preds_unlabeled), dim=0)\n        pseudo_labels = torch.softmax(preds_unlabeled / T, dim=1)\n    all_inputs = torch.cat([x_labeled, x_unlabeled], dim=0)\n    all_labels = torch.cat([y_labeled, pseudo_labels.argmax(dim=1)], dim=0)\n    lam = np.random.beta(0.75, 0.75)\n    index = torch.randperm(all_inputs.size(0))\n    mixed_inputs = lam * all_inputs + (1 - lam) * all_inputs[index]\n    mixed_labels = lam * all_labels + (1 - lam) * all_labels[index]\n    return mixed_inputs, mixed_labels\n\n# Updated Model evaluation with safeguards\ndef evaluate_model(model, test_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n\n    # Convert to numpy arrays for easier handling\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n\n    # Get the unique classes from predictions\n    unique_classes = np.unique(all_labels)\n    print(\"Unique classes in test set:\", unique_classes)\n\n    # Use only the unique classes for the classification report\n    print(classification_report(all_labels, all_preds, target_names=[full_dataset.classes[i] for i in unique_classes]))\n\n# Instantiate the pre-trained model\npretrained_model = get_pretrained_model().to(device)\n\n# Train and evaluate FixMatch model\nfixmatch_model = FixMatch(pretrained_model).to(device)\noptimizer = optim.Adam(fixmatch_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_model(fixmatch_model.model, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=15)\nprint(\"FixMatch Model Evaluation:\")\nevaluate_model(fixmatch_model.model, test_loader)\n\n# Train and evaluate MixMatch model\nmixmatch_model = get_pretrained_model().to(device)  # New model for MixMatch\noptimizer = optim.Adam(mixmatch_model.parameters(), lr=0.001)\n\nfor epoch in range(5):\n    mixmatch_model.train()\n    total_loss = 0\n    for (labeled_images, labeled_targets), (unlabeled_images, _) in zip(labeled_loader, unlabeled_loader):\n        labeled_images, labeled_targets = labeled_images.to(device), labeled_targets.to(device)\n        unlabeled_images = unlabeled_images.to(device)\n\n        optimizer.zero_grad()\n\n        # Apply MixMatch\n        mixed_inputs, mixed_labels = mixmatch(labeled_images, labeled_targets, unlabeled_images, mixmatch_model)\n        outputs = mixmatch_model(mixed_inputs)\n\n        loss = criterion(outputs, mixed_labels.long())\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f'Epoch [{epoch+1}/5], Loss: {total_loss / len(labeled_loader):.4f}')\n\nprint(\"MixMatch Model Evaluation:\")\nevaluate_model(mixmatch_model, test_loader)\n\n\n# # Train and evaluate Mean Teacher model\n# student_model = get_pretrained_model().to(device)\n# teacher_model = get_pretrained_model().to(device)\n# mean_teacher_model = MeanTeacher(student_model, teacher_model).to(device)\n\n# train_model(mean_teacher_model.student, optimizer, criterion, labeled_loader, unlabeled_loader, epochs=5)\n# print(\"Mean Teacher Model Evaluation:\")\n# evaluate_model(mean_teacher_model.student, test_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:11:37.897764Z","iopub.execute_input":"2024-09-27T20:11:37.898750Z","iopub.status.idle":"2024-09-27T20:18:35.252737Z","shell.execute_reply.started":"2024-09-27T20:11:37.898689Z","shell.execute_reply":"2024-09-27T20:18:35.251676Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15], Loss: 1.8266\nEpoch [2/15], Loss: 1.1422\nEpoch [3/15], Loss: 0.8473\nEpoch [4/15], Loss: 0.7180\nEpoch [5/15], Loss: 0.7266\nEpoch [6/15], Loss: 0.5416\nEpoch [7/15], Loss: 0.5742\nEpoch [8/15], Loss: 0.4859\nEpoch [9/15], Loss: 0.4466\nEpoch [10/15], Loss: 0.5161\nEpoch [11/15], Loss: 0.4262\nEpoch [12/15], Loss: 0.4203\nEpoch [13/15], Loss: 0.3979\nEpoch [14/15], Loss: 0.4740\nEpoch [15/15], Loss: 0.3842\nFixMatch Model Evaluation:\nUnique classes in test set: [0 1 2 3 4]\n              precision    recall  f1-score   support\n\n          H1       0.78      0.83      0.81       881\n          H2       0.52      0.46      0.49       467\n          H3       0.59      0.65      0.62       164\n          H5       0.79      0.81      0.80       163\n          H6       0.84      0.72      0.78       148\n\n    accuracy                           0.71      1823\n   macro avg       0.71      0.70      0.70      1823\nweighted avg       0.70      0.71      0.71      1823\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Loss: 0.8416\nEpoch [2/5], Loss: 0.6434\nEpoch [3/5], Loss: 0.6050\nEpoch [4/5], Loss: 0.5533\nEpoch [5/5], Loss: 0.5588\nMixMatch Model Evaluation:\nUnique classes in test set: [0 1 2 3 4]\n              precision    recall  f1-score   support\n\n          H1       0.56      1.00      0.72       881\n          H2       0.00      0.00      0.00       467\n          H3       0.08      0.04      0.05       164\n          H5       0.19      0.20      0.19       163\n          H6       0.00      0.00      0.00       148\n\n    accuracy                           0.50      1823\n   macro avg       0.17      0.25      0.19      1823\nweighted avg       0.30      0.50      0.37      1823\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]}]}